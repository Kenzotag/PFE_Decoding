{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "395509cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte original : Hello!\n",
      "Texte en binaire : 01001000 01100101 01101100 01101100 01101111 00100001\n",
      "Texte d√©cod√© : Hello!\n"
     ]
    }
   ],
   "source": [
    "#codage source UTF-8\n",
    "\n",
    "\n",
    "def text_to_binary(text):\n",
    "    return ' '.join(format(ord(char), '08b') for char in text)\n",
    "\n",
    "def binary_to_text(binary):\n",
    "    return ''.join(chr(int(b, 2)) for b in binary.split())\n",
    "\n",
    "# Exemple d'utilisation\n",
    "message = \"Hello!\"\n",
    "binary_message = text_to_binary(message)\n",
    "decoded_message = binary_to_text(binary_message)\n",
    "\n",
    "print(\"Texte original :\", message)\n",
    "print(\"Texte en binaire :\", binary_message)\n",
    "print(\"Texte d√©cod√© :\", decoded_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010818ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a15a38cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\CyberVortex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyldpc' has no attribute 'create_parity_check_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m d_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Degr√© des colonnes\u001b[39;00m\n\u001b[0;32m     20\u001b[0m d_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Degr√© des lignes\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m H \u001b[38;5;241m=\u001b[39m pyldpc\u001b[38;5;241m.\u001b[39mcreate_parity_check_matrix(n, d_v, d_c, systematic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calcul de la matrice g√©n√©ratrice G\u001b[39;00m\n\u001b[0;32m     23\u001b[0m G \u001b[38;5;241m=\u001b[39m pyldpc\u001b[38;5;241m.\u001b[39mcompute_generator_matrix(H)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyldpc' has no attribute 'create_parity_check_matrix'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyldpc\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "\n",
    "# T√©l√©charger le dictionnaire de mots fran√ßais\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "# S√©lection des mots fran√ßais\n",
    "mots_francais = [mot.lower() for mot in words.words() if mot.isalpha()]\n",
    "\n",
    "# Param√®tres du codage LDPC\n",
    "n = 64  # Longueur du bloc LDPC cod√©\n",
    "k = 32  # Longueur du message original avant codage\n",
    "# Cr√©ation de la matrice de contr√¥le de parit√© H\n",
    "d_v = 2  # Degr√© des colonnes\n",
    "d_c = 4  # Degr√© des lignes\n",
    "H = pyldpc.create_parity_check_matrix(n, d_v, d_c, systematic=True)\n",
    "# Calcul de la matrice g√©n√©ratrice G\n",
    "G = pyldpc.compute_generator_matrix(H)\n",
    "\n",
    "# Plage de SNR et nombre d'√©chantillons\n",
    "snr_db_range = np.arange(0, 11, 2)\n",
    "nombre_echantillons_par_snr = 1000\n",
    "\n",
    "# Base de donn√©es\n",
    "train_data_bruitee = []\n",
    "train_data_originale = []\n",
    "\n",
    "# Fonction pour convertir un mot en binaire (ASCII)\n",
    "def mot_en_binaire(mot):\n",
    "    binaire = ''.join(format(ord(c), '08b') for c in mot)  # ASCII en binaire\n",
    "    return np.array([int(b) for b in binaire])\n",
    "\n",
    "# G√©n√©ration de la base de donn√©es\n",
    "for snr_db in snr_db_range:\n",
    "    print(f\"üì° G√©n√©ration des donn√©es pour SNR = {snr_db} dB...\")\n",
    "    \n",
    "    for _ in range(nombre_echantillons_par_snr):\n",
    "        # 1. Choisir un mot al√©atoire en fran√ßais\n",
    "        mot_original = random.choice(mots_francais)\n",
    "        \n",
    "        # 2. Convertir en binaire\n",
    "        mot_original_binaire = mot_en_binaire(mot_original)\n",
    "\n",
    "        # 3. Adapter √† la taille k\n",
    "        if len(mot_original_binaire) < k:\n",
    "            mot_original_binaire = np.pad(mot_original_binaire, (0, k - len(mot_original_binaire)), 'constant')\n",
    "        elif len(mot_original_binaire) > k:\n",
    "            mot_original_binaire = mot_original_binaire[:k]\n",
    "\n",
    "        mot_original_binaire_float = mot_original_binaire.astype(np.float64)\n",
    "\n",
    "        # 4. Encodage LDPC\n",
    "        mot_code_ldpc = pyldpc.encode(G, mot_original_binaire_float)\n",
    "\n",
    "        # 5. Ajout du bruit (AWGN)\n",
    "        signal_puissance = np.mean(mot_code_ldpc ** 2)\n",
    "        snr_lin = 10 ** (snr_db / 10)\n",
    "        bruit_puissance = signal_puissance / snr_lin\n",
    "        bruit_std = np.sqrt(bruit_puissance)\n",
    "        bruit_gaussien = np.random.normal(0, bruit_std, size=len(mot_code_ldpc))\n",
    "        \n",
    "        mot_code_bruite = mot_code_ldpc + bruit_gaussien\n",
    "\n",
    "        # 6. Stockage\n",
    "        train_data_bruitee.append(mot_code_bruite)\n",
    "        train_data_originale.append(mot_original_binaire_float)\n",
    "\n",
    "# Conversion en NumPy\n",
    "train_data_bruitee = np.array(train_data_bruitee)\n",
    "train_data_originale = np.array(train_data_originale)\n",
    "\n",
    "# Sauvegarde en .npy\n",
    "np.save(\"ldpc_data_bruitee.npy\", train_data_bruitee)\n",
    "np.save(\"ldpc_data_originale.npy\", train_data_originale)\n",
    "\n",
    "# V√©rification\n",
    "print(\"\\n‚úÖ Base de donn√©es g√©n√©r√©e et sauvegard√©e avec succ√®s !\")\n",
    "print(f\"Nombre total d'√©chantillons : {len(train_data_bruitee)}\")\n",
    "print(f\"Forme des donn√©es bruit√©es : {train_data_bruitee.shape}\")\n",
    "print(f\"Forme des donn√©es originales : {train_data_originale.shape}\")\n",
    "\n",
    "# Exemple d'affichage\n",
    "print(\"\\nüîç Exemples de donn√©es :\")\n",
    "for i in range(2):  # Afficher 2 √©chantillons\n",
    "    print(f\"\\nTrame bruit√©e [{i}]: {train_data_bruitee[i][:10]} ...\")  # Affiche les 10 premiers √©l√©ments\n",
    "    print(f\"Trame originale [{i}]: {train_data_originale[i]}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6974e166",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ldpc_data_bruitee.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Chargement des donn√©es\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldpc_data_bruitee.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldpc_data_originale.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExemple de trame bruit√©e :\u001b[39m\u001b[38;5;124m\"\u001b[39m, X[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ldpc_data_bruitee.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Chargement des donn√©es\n",
    "X = np.load(\"ldpc_data_bruitee.npy\")\n",
    "Y = np.load(\"ldpc_data_originale.npy\")\n",
    "\n",
    "print(\"Exemple de trame bruit√©e :\", X[0])\n",
    "print(\"Exemple de trame originale :\", Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e66a82f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des donn√©es pour SNR = 0 dB...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ldpc.codes' has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m msg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, k, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# üü¢ 2. Encodage LDPC\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m codeword \u001b[38;5;241m=\u001b[39m ldpc\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mencode(H, msg)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# üü¢ 3. Ajout du bruit AWGN\u001b[39;00m\n\u001b[0;32m     41\u001b[0m snr_lin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(snr_db \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Conversion dB -> lin√©aire\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ldpc.codes' has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ldpc.codes\n",
    "from ldpc.bplsd_decoder import BpLsdDecoder\n",
    "import pickle\n",
    "\n",
    "# üîπ 1. G√©n√©ration de la matrice de contr√¥le de parit√© H\n",
    "H = ldpc.codes.hamming_code(5)  # Code de Hamming (31,26) => n=31, k=26\n",
    "n = H.shape[1]  # Longueur du mot de code\n",
    "k = n - H.shape[0]  # Longueur du message original\n",
    "\n",
    "# üîπ 2. Initialisation du d√©codeur BP-LSD\n",
    "decoder = BpLsdDecoder(\n",
    "    H,\n",
    "    error_rate=0.1,\n",
    "    bp_method='product_sum',\n",
    "    max_iter=10,\n",
    "    schedule='serial',\n",
    "    lsd_method='lsd_cs',\n",
    "    lsd_order=0\n",
    ")\n",
    "\n",
    "# üîπ 3. Param√®tres du bruit et de la base de donn√©es\n",
    "snr_db_range = np.arange(0, 11, 2)  # SNR de 0dB √† 10dB\n",
    "nb_samples = 1000  # Nombre d'√©chantillons\n",
    "\n",
    "# üîπ 4. Listes pour stocker les donn√©es\n",
    "dataset = []\n",
    "\n",
    "# üîπ 5. G√©n√©ration des √©chantillons\n",
    "for snr_db in snr_db_range:\n",
    "    print(f\"G√©n√©ration des donn√©es pour SNR = {snr_db} dB...\")\n",
    "    \n",
    "    for _ in range(nb_samples):\n",
    "        # üü¢ 1. G√©n√©rer un message binaire al√©atoire\n",
    "        msg = np.random.randint(0, 2, k, dtype=np.uint8)\n",
    "        \n",
    "        # üü¢ 2. Encodage LDPC\n",
    "        codeword = ldpc.codes.encode(H, msg)\n",
    "        \n",
    "        # üü¢ 3. Ajout du bruit AWGN\n",
    "        snr_lin = 10**(snr_db / 10)  # Conversion dB -> lin√©aire\n",
    "        noise_std = np.sqrt(1 / (2 * snr_lin))  # √âcart-type du bruit\n",
    "        noise = np.random.normal(0, noise_std, codeword.shape)\n",
    "        noisy_codeword = codeword + noise\n",
    "        \n",
    "        # üü¢ 4. Stockage des donn√©es\n",
    "        dataset.append((noisy_codeword, msg))\n",
    "\n",
    "# üîπ 6. Exportation de la base de donn√©es\n",
    "with open(\"ldpc_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "print(\"\\n‚úÖ Base de donn√©es g√©n√©r√©e et sauvegard√©e sous 'ldpc_dataset.pkl' !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1234ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des donn√©es pour SNR = 0 dB...\n",
      "G√©n√©ration des donn√©es pour SNR = 2 dB...\n",
      "G√©n√©ration des donn√©es pour SNR = 4 dB...\n",
      "G√©n√©ration des donn√©es pour SNR = 6 dB...\n",
      "G√©n√©ration des donn√©es pour SNR = 8 dB...\n",
      "G√©n√©ration des donn√©es pour SNR = 10 dB...\n",
      "\n",
      "‚úÖ Base de donn√©es sauvegard√©e sous 'ldpc_dataset.pkl' !\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyldpc import make_ldpc, encode, decode, get_message\n",
    "import pickle\n",
    "\n",
    "# üîπ 1. D√©finition des param√®tres LDPC\n",
    "n = 15  # Longueur du mot de code\n",
    "d_v = 4  # Degr√© des n≈ìuds de variable\n",
    "d_c = 5  # Degr√© des n≈ìuds de contr√¥le\n",
    "snr_db_range = np.arange(0, 11, 2)  # SNR de 0dB √† 10dB\n",
    "nb_samples = 1000  # Nombre d'√©chantillons\n",
    "\n",
    "# üîπ 2. G√©n√©ration des matrices LDPC\n",
    "H, G = make_ldpc(n, d_v, d_c, systematic=True, sparse=True)\n",
    "k = G.shape[1]  # Taille du message original\n",
    "\n",
    "# üîπ 3. Base de donn√©es\n",
    "dataset = []\n",
    "\n",
    "# üîπ 4. G√©n√©ration des √©chantillons\n",
    "for snr_db in snr_db_range:\n",
    "    print(f\"G√©n√©ration des donn√©es pour SNR = {snr_db} dB...\")\n",
    "    \n",
    "    for _ in range(nb_samples):\n",
    "        # üü¢ 1. G√©n√©rer un message binaire al√©atoire\n",
    "        v = np.random.randint(2, size=k)\n",
    "\n",
    "        # üü¢ 2. Encodage LDPC\n",
    "        y = encode(G, v, snr_db)\n",
    "\n",
    "        # üü¢ 3. D√©codage\n",
    "        d = decode(H, y, snr_db)\n",
    "\n",
    "        # üü¢ 4. Extraction du message original\n",
    "        x = get_message(G, d)\n",
    "\n",
    "        # üü¢ 5. Stockage\n",
    "        dataset.append((y, v))\n",
    "\n",
    "# üîπ 5. Sauvegarde de la base de donn√©es\n",
    "with open(\"ldpc_dataset.csv\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "print(\"\\n‚úÖ Base de donn√©es sauvegard√©e sous 'ldpc_dataset.pkl' !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ff3673b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() missing 1 required positional argument: 'snr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m mot_original_binaire_float \u001b[38;5;241m=\u001b[39m mot_original_binaire\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)  \u001b[38;5;66;03m# Conversion en float\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 3. Encodage LDPC\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m mot_code_ldpc \u001b[38;5;241m=\u001b[39m encode(G, mot_original_binaire)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 4. Ajouter du bruit gaussien\u001b[39;00m\n\u001b[0;32m     41\u001b[0m signal_puissance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mot_code_ldpc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'snr'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyldpc import make_ldpc, encode, decode, get_message\n",
    "\n",
    "# Param√®tres du codage LDPC\n",
    "n = 15  # Longueur du bloc cod√©\n",
    "d_v = 4  # Degr√© des variables\n",
    "d_c = 5  # Degr√© des contr√¥les\n",
    "snr = 20  # SNR en dB\n",
    "H, G = make_ldpc(n, d_v, d_c, systematic=True, sparse=True)\n",
    "\n",
    "k = G.shape[1]  # Longueur du message original\n",
    "longueur_min_mot = 10\n",
    "longueur_max_mot = 50\n",
    "\n",
    "# Nombre d'√©chantillons\n",
    "nombre_echantillons = 1000\n",
    "\n",
    "# Listes pour stocker les donn√©es\n",
    "train_data_bruitee = []\n",
    "train_data_originale = []\n",
    "\n",
    "# G√©n√©ration des √©chantillons\n",
    "for _ in range(nombre_echantillons):\n",
    "    # 1. G√©n√©rer un mot binaire original al√©atoire de longueur variable\n",
    "    longueur_mot_original = np.random.randint(longueur_min_mot, longueur_max_mot + 1)\n",
    "    mot_original_binaire = np.random.randint(0, 2, longueur_mot_original)\n",
    "    \n",
    "    # 2. Padding du mot original pour correspondre √† la longueur k\n",
    "    if len(mot_original_binaire) < k:\n",
    "        mot_original_binaire = np.pad(mot_original_binaire, (0, k - len(mot_original_binaire)), 'constant')\n",
    "    elif len(mot_original_binaire) > k:\n",
    "        mot_original_binaire = mot_original_binaire[:k]  # Tronquer si plus long que k\n",
    "\n",
    "    mot_original_binaire_float = mot_original_binaire.astype(np.float64)  # Conversion en float\n",
    "\n",
    "    # 3. Encodage LDPC\n",
    "    mot_code_ldpc = encode(G, mot_original_binaire)\n",
    "\n",
    "    # 4. Ajouter du bruit gaussien\n",
    "    signal_puissance = np.mean(mot_code_ldpc**2)\n",
    "    snr_lin = 10**(snr/10)\n",
    "    bruit_puissance = signal_puissance / snr_lin\n",
    "    bruit_std = np.sqrt(bruit_puissance)\n",
    "    bruit_gaussien = np.random.normal(0, bruit_std, size=len(mot_code_ldpc))\n",
    "\n",
    "    # 5. Ajouter le bruit au mot cod√©\n",
    "    mot_code_bruite = mot_code_ldpc + bruit_gaussien\n",
    "\n",
    "    # 6. Stocker les donn√©es dans les listes\n",
    "    train_data_bruitee.append(mot_code_bruite)\n",
    "    train_data_originale.append(mot_original_binaire_float)\n",
    "\n",
    "# Conversion des listes en tableaux NumPy pour pouvoir les manipuler facilement\n",
    "train_data_bruitee = np.array(train_data_bruitee)\n",
    "train_data_originale = np.array(train_data_originale)\n",
    "\n",
    "# Conversion en DataFrame pour faciliter l'exportation en CSV\n",
    "data_dict = {\n",
    "    \"Trame cod√©e bruit√©e\": [', '.join(map(str, frame)) for frame in train_data_bruitee],\n",
    "    \"Trame binaire originale\": [', '.join(map(str, frame)) for frame in train_data_originale]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Exporter le DataFrame au format CSV\n",
    "df.to_csv(\"base_de_donnees_ldpc.csv\", index=False)\n",
    "\n",
    "print(\"\\nBase de donn√©es g√©n√©r√©e et export√©e en CSV avec succ√®s!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a339a85",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() missing 1 required positional argument: 'snr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m message_original_binaire \u001b[38;5;241m=\u001b[39m generate_binary_frame(k)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 2. Encodage LDPC\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m mot_code_ldpc \u001b[38;5;241m=\u001b[39m encode(G, message_original_binaire) \u001b[38;5;66;03m# Argument snr supprim√© ici\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 3. Ajouter du bruit gaussien\u001b[39;00m\n\u001b[0;32m     49\u001b[0m mot_code_bruite \u001b[38;5;241m=\u001b[39m add_gaussian_noise(mot_code_ldpc, snr_db)\n",
      "\u001b[1;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'snr'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyldpc import make_ldpc, encode\n",
    "import random\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# D√©finir les param√®tres pour la g√©n√©ration de la base de donn√©es\n",
    "n = 15   # Longueur du bloc cod√© LDPC\n",
    "dv = 4   # Degr√© des n≈ìuds variables pour LDPC\n",
    "dc = 5   # Degr√© des n≈ìuds de contr√¥le pour LDPC\n",
    "snr_db = 5  # SNR en dB pour le bruit gaussien (vous pourrez varier cela)\n",
    "nombre_echantillons = 2000  # Nombre total d'√©chantillons √† g√©n√©rer\n",
    "test_size = 0.2  # Proportion des donn√©es pour l'ensemble de test\n",
    "validation_size = 0.2 # Proportion des donn√©es pour l'ensemble de validation (apr√®s avoir s√©par√© le test)\n",
    "\n",
    "\n",
    "# G√©n√©rer les matrices LDPC une seule fois (en dehors de la boucle)\n",
    "H, G = make_ldpc(n, dv, dc, systematic=True, sparse=True)\n",
    "k = G.shape[1]  # Longueur du message original (bits d'information)\n",
    "\n",
    "# Fonction pour g√©n√©rer une trame binaire al√©atoire de longueur k (pour l'entra√Ænement DL, on utilise des trames binaires fixes)\n",
    "def generate_binary_frame(length):\n",
    "    return np.random.randint(0, 2, size=length, dtype=np.uint8) # directement en uint8 pour efficacit√©\n",
    "\n",
    "# Fonction pour ajouter du bruit gaussien\n",
    "def add_gaussian_noise(signal, snr_db):\n",
    "    signal_power = np.mean(signal**2)\n",
    "    snr_linear = 10**(snr_db/10)\n",
    "    noise_power = signal_power / snr_linear\n",
    "    noise_std = np.sqrt(noise_power)\n",
    "    noise = np.random.normal(0, noise_std, size=len(signal))\n",
    "    return signal + noise\n",
    "\n",
    "# Listes pour stocker les donn√©es\n",
    "input_data_noisy_frames = [] # Pour les trames cod√©es bruit√©es (entr√©e du d√©codeur DL)\n",
    "output_data_original_frames = [] # Pour les trames binaires originales (sortie cible du d√©codeur DL)\n",
    "labels = [] # Pour les mots originaux (si vous souhaitez garder les mots comme labels)\n",
    "\n",
    "## G√©n√©ration des √©chantillons\n",
    "for i in range(nombre_echantillons):\n",
    "    # 1. G√©n√©rer une trame binaire al√©atoire de longueur k (bits d'information)\n",
    "    message_original_binaire = generate_binary_frame(k)\n",
    "\n",
    "    # 2. Encodage LDPC\n",
    "    mot_code_ldpc = encode(G, message_original_binaire) # Argument snr supprim√© ici\n",
    "\n",
    "    # 3. Ajouter du bruit gaussien\n",
    "    mot_code_bruite = add_gaussian_noise(mot_code_ldpc, snr_db)\n",
    "\n",
    "    # 4. Stocker les donn√©es (utiliser des types NumPy directement pour efficacit√©)\n",
    "    input_data_noisy_frames.append(mot_code_bruite)\n",
    "    output_data_original_frames.append(message_original_binaire)\n",
    "    labels.append(f\"sample_{i+1}\") # Etiquette simple pour chaque √©chantillon\n",
    "    \n",
    "# Conversion des listes en tableaux NumPy (d√©j√† fait en append mais redondant pour s'assurer)\n",
    "input_data_noisy_frames = np.array(input_data_noisy_frames)\n",
    "output_data_original_frames = np.array(output_data_original_frames)\n",
    "\n",
    "# S√©paration des donn√©es en ensembles d'entra√Ænement, validation et test\n",
    "# 1. S√©parer d'abord l'ensemble de test\n",
    "train_val_input, test_input, train_val_output, test_output = train_test_split(\n",
    "    input_data_noisy_frames, output_data_original_frames, test_size=test_size, random_state=42\n",
    ")\n",
    "\n",
    "# 2. S√©parer l'ensemble d'entra√Ænement et de validation √† partir du reste (train_val)\n",
    "train_input, val_input, train_output, val_output = train_test_split(\n",
    "    train_val_input, train_val_output, test_size=validation_size / (1 - test_size), random_state=42 # Ajuster la taille de validation\n",
    ")\n",
    "\n",
    "# Cr√©er des DataFrames pandas pour chaque ensemble pour faciliter l'exportation en CSV (optionnel, peut √™tre sauvegard√© directement en .npy)\n",
    "def create_dataframe(input_frames, output_frames, set_name):\n",
    "    data_dict = {\n",
    "        \"Trame cod√©e bruit√©e\": [', '.join(map(str, frame)) for frame in input_frames],\n",
    "        \"Trame binaire originale\": [', '.join(map(str, frame)) for frame in output_frames]\n",
    "    }\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    return df\n",
    "\n",
    "train_df = create_dataframe(train_input, train_output, \"train\")\n",
    "val_df = create_dataframe(val_input, val_output, \"validation\")\n",
    "test_df = create_dataframe(test_input, test_output, \"test\")\n",
    "\n",
    "\n",
    "# Exporter les DataFrames au format CSV dans des dossiers s√©par√©s\n",
    "import os\n",
    "output_dir = \"base_de_donnees_ldpc_binaire\"\n",
    "os.makedirs(output_dir, exist_ok=True) # Cr√©er le dossier principal si inexistant\n",
    "\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"validation\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "train_df.to_csv(os.path.join(train_dir, \"train_data_ldpc_binaire.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(val_dir, \"validation_data_ldpc_binaire.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(test_dir, \"test_data_ldpc_binaire.csv\"), index=False)\n",
    "\n",
    "\n",
    "# (Option alternative pour sauvegarder en .npy directement - plus efficace pour les donn√©es num√©riques pour DL)\n",
    "# np.save(os.path.join(train_dir, \"train_input_noisy.npy\"), train_input)\n",
    "# np.save(os.path.join(train_dir, \"train_output_original.npy\"), train_output)\n",
    "# np.save(os.path.join(val_dir, \"val_input_noisy.npy\"), val_input)\n",
    "# np.save(os.path.join(val_dir, \"val_output_original.npy\"), val_output)\n",
    "# np.save(os.path.join(test_dir, \"test_input_noisy.npy\"), test_input)\n",
    "# np.save(os.path.join(test_dir, \"test_output_original.npy\"), test_output)\n",
    "\n",
    "\n",
    "print(f\"\\nBase de donn√©es binaire LDPC g√©n√©r√©e avec {nombre_echantillons} √©chantillons.\")\n",
    "print(f\"Ensembles de donn√©es CSV export√©s dans les dossiers: '{output_dir}/train', '{output_dir}/validation', '{output_dir}/test'\")\n",
    "print(f\"Param√®tres utilis√©s: Longueur bloc cod√© n={n}, Longueur message k={k}, SNR={snr_db}dB\")\n",
    "print(f\"R√©partition: Train={len(train_input)}, Validation={len(val_input)}, Test={len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4e0c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base de donn√©es g√©n√©r√©e et export√©e en CSV avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyldpc import make_ldpc, encode\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Param√®tres du codage LDPC\n",
    "n = 15  # Longueur du bloc cod√©\n",
    "d_v = 4  # Degr√© des variables\n",
    "d_c = 5  # Degr√© des contr√¥les\n",
    "snr = 20  # SNR en dB\n",
    "H, G = make_ldpc(n, d_v, d_c, systematic=True, sparse=True)\n",
    "\n",
    "k = G.shape[1]  # Longueur du message original\n",
    "\n",
    "# Fonction pour g√©n√©rer un mot al√©atoire de longueur variable\n",
    "def random_word(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "# Fonction pour convertir un mot en binaire (UTF-8 ou ASCII)\n",
    "def word_to_binary(word, encoding=\"utf-8\"):\n",
    "    return np.array([int(bit) for bit in ''.join(format(ord(char), '08b') for char in word)], dtype=np.uint8)\n",
    "\n",
    "# Longueur des mots al√©atoires (entre 0 et 10)\n",
    "longueur_min_mot = 0\n",
    "longueur_max_mot = 10\n",
    "\n",
    "# Nombre d'√©chantillons\n",
    "nombre_echantillons = 1000\n",
    "\n",
    "# Listes pour stocker les donn√©es\n",
    "train_data_bruitee = []\n",
    "train_data_originale = []\n",
    "labels = []\n",
    "\n",
    "# G√©n√©ration des √©chantillons\n",
    "for _ in range(nombre_echantillons):\n",
    "    # 1. G√©n√©rer un mot al√©atoire de longueur entre 0 et 10\n",
    "    longueur_mot = random.randint(longueur_min_mot, longueur_max_mot)\n",
    "    mot_original = random_word(longueur_mot)\n",
    "    \n",
    "    # 2. Convertir le mot en binaire\n",
    "    mot_original_binaire = word_to_binary(mot_original)\n",
    "    \n",
    "    # 3. Padding du mot original pour correspondre √† la longueur k\n",
    "    if len(mot_original_binaire) < k:\n",
    "        mot_original_binaire = np.pad(mot_original_binaire, (0, k - len(mot_original_binaire)), 'constant')\n",
    "    elif len(mot_original_binaire) > k:\n",
    "        mot_original_binaire = mot_original_binaire[:k]  # Tronquer si plus long que k\n",
    "    \n",
    "    mot_original_binaire_float = mot_original_binaire.astype(np.float64)  # Conversion en float\n",
    "\n",
    "    # 4. Encodage LDPC\n",
    "    mot_code_ldpc = encode(G, mot_original_binaire, snr)\n",
    "\n",
    "    # 5. Ajouter du bruit gaussien\n",
    "    signal_puissance = np.mean(mot_code_ldpc**2)\n",
    "    snr_lin = 10**(snr/10)\n",
    "    bruit_puissance = signal_puissance / snr_lin\n",
    "    bruit_std = np.sqrt(bruit_puissance)\n",
    "    bruit_gaussien = np.random.normal(0, bruit_std, size=len(mot_code_ldpc))\n",
    "\n",
    "    # 6. Ajouter le bruit au mot cod√©\n",
    "    mot_code_bruite = mot_code_ldpc + bruit_gaussien\n",
    "\n",
    "    # 7. Stocker les donn√©es dans les listes\n",
    "    train_data_bruitee.append(mot_code_bruite)\n",
    "    train_data_originale.append(mot_original_binaire_float)\n",
    "    labels.append(mot_original)  # Ajouter l'√©tiquette\n",
    "\n",
    "# Conversion des listes en tableaux NumPy pour pouvoir les manipuler facilement\n",
    "train_data_bruitee = np.array(train_data_bruitee)\n",
    "train_data_originale = np.array(train_data_originale)\n",
    "\n",
    "# Conversion en DataFrame pour faciliter l'exportation en CSV\n",
    "data_dict = {\n",
    "    \"Mot original\": labels,  # Mot original comme √©tiquette\n",
    "    \"Trame cod√©e bruit√©e\": [', '.join(map(str, frame)) for frame in train_data_bruitee],\n",
    "    \"Trame binaire originale\": [', '.join(map(str, frame)) for frame in train_data_originale]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Exporter le DataFrame au format CSV\n",
    "df.to_csv(\"base_de_donnees_ldpc_mots_aleatoires.csv\", index=False)\n",
    "\n",
    "print(\"\\nBase de donn√©es g√©n√©r√©e et export√©e en CSV avec succ√®s!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b6efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
